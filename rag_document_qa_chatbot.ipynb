{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ RAG-Based Document QA Chatbot\n",
    "\n",
    "This notebook implements a **Retrieval-Augmented Generation (RAG)** system for document-based question answering with an attractive UI.\n",
    "\n",
    "## Features:\n",
    "- üìÑ PDF document processing and chunking\n",
    "- üîç Semantic search using embeddings\n",
    "- üß† Integration with free LLM APIs (Hugging Face)\n",
    "- üé® Beautiful and modern UI using Gradio\n",
    "- ‚ö° Fast and efficient retrieval system\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Installation and Setup\n",
    "\n",
    "First, let's install all the required dependencies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (5.25.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.56.2)\n",
      "Requirement already satisfied: torch in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (5.1.1)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: langchain in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.1.7)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: chromadb in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (3.1.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (0.35.1)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (0.117.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (3.7.1)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (4.14.1)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: pydub in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (10.1.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (0.48.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (0.35.0)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (0.6.0)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (0.12.3)\n",
      "Requirement already satisfied: groovy~=0.1 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: gradio-client==1.8.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (1.8.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (0.16.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (23.2)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (2.11.9)\n",
      "Requirement already satisfied: fsspec in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio-client==1.8.0->gradio) (2025.5.1)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: requests in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sentence-transformers) (1.7.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (0.6.7)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.22 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (0.1.23)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.20 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (0.0.20)\n",
      "Requirement already satisfied: langsmith<0.1,>=0.0.83 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (0.0.87)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (3.12.14)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (1.35.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (1.22.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (14.0.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (1.35.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (4.25.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (1.73.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (1.4.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (1.35.0)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.20.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.6.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.7.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.6.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.2)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: tomli>=1.1.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from build>=1.0.3->chromadb) (2.2.1)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.24.1->gradio) (2025.7.9)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (3.0.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.1)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (6.32.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.35.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.35.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.35.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.35.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.56b0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.56b0)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn>=0.14.0->gradio) (1.1.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn>=0.14.0->gradio) (1.1.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn>=0.14.0->gradio) (0.6.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.1.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.35.1)\n",
      "Requirement already satisfied: requests in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (2.32.4)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (4.14.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface_hub) (2025.5.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub) (2025.7.9)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface_hub) (2.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\aisha\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install gradio transformers torch sentence-transformers PyPDF2 faiss-cpu numpy pandas langchain tiktoken chromadb\n",
    "!pip install --upgrade huggingface_hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Import Libraries and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'VersionComparison' from 'transformers.utils.import_utils' (C:\\Users\\aisha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\__init__.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m     export_dynamic_quantized_onnx_model,\n\u001b[0;32m     12\u001b[0m     export_optimized_onnx_model,\n\u001b[0;32m     13\u001b[0m     export_static_quantized_openvino_model,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     CrossEncoder,\n\u001b[0;32m     17\u001b[0m     CrossEncoderModelCardData,\n\u001b[0;32m     18\u001b[0m     CrossEncoderTrainer,\n\u001b[0;32m     19\u001b[0m     CrossEncoderTrainingArguments,\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\backend\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m__future__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_onnx_model, load_openvino_model\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m export_optimized_onnx_model\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m export_dynamic_quantized_onnx_model, export_static_quantized_openvino_model\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sentence_transformers\\backend\\load.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _save_pretrained_wrapper, backend_should_export, backend_warn_to_save\n\u001b[0;32m     11\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\configuration_utils.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m custom_object_save\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_gguf_pytorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_gguf_checkpoint\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     29\u001b[0m     CONFIG_NAME,\n\u001b[0;32m     30\u001b[0m     PushToHubMixin,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     logging,\n\u001b[0;32m     38\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\dynamic_module_utils.py:44\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     37\u001b[0m     HF_MODULES_CACHE,\n\u001b[0;32m     38\u001b[0m     TRANSFORMERS_DYNAMIC_MODULE_NAME,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     logging,\n\u001b[0;32m     43\u001b[0m )\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimport_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VersionComparison, split_package_version\n\u001b[0;32m     47\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)  \u001b[38;5;66;03m# pylint: disable=invalid-name\u001b[39;00m\n\u001b[0;32m     48\u001b[0m _HF_REMOTE_CODE_LOCK \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mLock()\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'VersionComparison' from 'transformers.utils.import_utils' (C:\\Users\\aisha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py)"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import PyPDF2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from typing import List, Tuple, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")\n",
    "print(f\"üî• PyTorch version: {torch.__version__}\")\n",
    "print(f\"ü§ó Transformers available: True\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è RAG System Implementation\n",
    "\n",
    "Let's build our RAG system with the following components:\n",
    "1. **Document Processor**: Extract and chunk text from PDFs\n",
    "2. **Embedding Generator**: Create vector embeddings\n",
    "3. **Vector Store**: Store and retrieve similar chunks\n",
    "4. **LLM Integration**: Generate answers using retrieved context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\"Handles PDF, JSON, and TXT processing and text chunking\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.supported_formats = ['.pdf', '.json', '.txt']\n",
    "    \n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"Extract text from PDF file\"\"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                text = \"\"\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text() + \"\\n\"\n",
    "                return text\n",
    "        except Exception as e:\n",
    "            return f\"Error reading PDF: {str(e)}\"\n",
    "    \n",
    "    def extract_text_from_json(self, json_path: str) -> str:\n",
    "        \"\"\"Extract text from JSON file\"\"\"\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                \n",
    "            def extract_text_recursive(obj, text_parts=[]):\n",
    "                \"\"\"Recursively extract text from JSON object\"\"\"\n",
    "                if isinstance(obj, dict):\n",
    "                    for key, value in obj.items():\n",
    "                        if isinstance(value, str):\n",
    "                            text_parts.append(f\"{key}: {value}\")\n",
    "                        else:\n",
    "                            extract_text_recursive(value, text_parts)\n",
    "                elif isinstance(obj, list):\n",
    "                    for item in obj:\n",
    "                        extract_text_recursive(item, text_parts)\n",
    "                elif isinstance(obj, str):\n",
    "                    text_parts.append(obj)\n",
    "                return text_parts\n",
    "            \n",
    "            text_parts = extract_text_recursive(data)\n",
    "            return \"\\n\".join(text_parts)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error reading JSON: {str(e)}\"\n",
    "    \n",
    "    def extract_text_from_txt(self, txt_path: str) -> str:\n",
    "        \"\"\"Extract text from TXT file\"\"\"\n",
    "        try:\n",
    "            with open(txt_path, 'r', encoding='utf-8') as file:\n",
    "                return file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            # Try with different encoding if UTF-8 fails\n",
    "            try:\n",
    "                with open(txt_path, 'r', encoding='latin-1') as file:\n",
    "                    return file.read()\n",
    "            except Exception as e:\n",
    "                return f\"Error reading TXT file: {str(e)}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error reading TXT file: {str(e)}\"\n",
    "    \n",
    "    def extract_text_from_file(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text from file based on extension\"\"\"\n",
    "        file_ext = os.path.splitext(file_path)[1].lower()\n",
    "        \n",
    "        if file_ext == '.pdf':\n",
    "            return self.extract_text_from_pdf(file_path)\n",
    "        elif file_ext == '.json':\n",
    "            return self.extract_text_from_json(file_path)\n",
    "        elif file_ext == '.txt':\n",
    "            return self.extract_text_from_txt(file_path)\n",
    "        else:\n",
    "            return f\"Unsupported file format: {file_ext}. Supported formats: {', '.join(self.supported_formats)}\"\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        # Remove extra whitespace and normalize\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'\\n+', '\\n', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def chunk_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into overlapping chunks\"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(words), self.chunk_size - self.chunk_overlap):\n",
    "            chunk = ' '.join(words[i:i + self.chunk_size])\n",
    "            if len(chunk.strip()) > 50:  # Only keep substantial chunks\n",
    "                chunks.append(chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def process_document(self, file_path: str) -> List[str]:\n",
    "        \"\"\"Process document and return chunks\"\"\"\n",
    "        file_ext = os.path.splitext(file_path)[1].lower()\n",
    "        print(f\"üìÑ Processing {file_ext.upper()} document: {file_path}\")\n",
    "        \n",
    "        text = self.extract_text_from_file(file_path)\n",
    "        \n",
    "        # Check if extraction was successful\n",
    "        if text.startswith(\"Error\") or text.startswith(\"Unsupported\"):\n",
    "            raise ValueError(text)\n",
    "        \n",
    "        cleaned_text = self.clean_text(text)\n",
    "        chunks = self.chunk_text(cleaned_text)\n",
    "        print(f\"‚úÖ Created {len(chunks)} chunks from {file_ext.upper()} file\")\n",
    "        return chunks\n",
    "\n",
    "print(\"üìù Enhanced DocumentProcessor class created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    \"\"\"Complete RAG system for document QA\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.document_processor = DocumentProcessor()\n",
    "        self.embedding_model = None\n",
    "        self.vector_store = None\n",
    "        self.chunks = []\n",
    "        self.qa_pipeline = None\n",
    "        self.setup_models()\n",
    "    \n",
    "    def setup_models(self):\n",
    "        \"\"\"Initialize embedding model and QA pipeline\"\"\"\n",
    "        print(\"üöÄ Initializing models...\")\n",
    "        \n",
    "        # Load embedding model (lightweight and fast)\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        print(\"‚úÖ Embedding model loaded\")\n",
    "        \n",
    "        # For now, we'll use a simple context-based approach instead of a heavy LLM\n",
    "        print(\"‚úÖ Using context-based QA approach\")\n",
    "    \n",
    "    def load_document(self, pdf_path: str):\n",
    "        \"\"\"Load and process document\"\"\"\n",
    "        self.chunks = self.document_processor.process_document(pdf_path)\n",
    "        \n",
    "        if not self.chunks:\n",
    "            raise ValueError(\"No valid chunks extracted from document\")\n",
    "        \n",
    "        # Create embeddings\n",
    "        print(\"üîç Creating embeddings...\")\n",
    "        embeddings = self.embedding_model.encode(self.chunks)\n",
    "        \n",
    "        # Setup FAISS vector store\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.vector_store = faiss.IndexFlatIP(dimension)  # Inner product for similarity\n",
    "        \n",
    "        # Normalize embeddings for cosine similarity\n",
    "        faiss.normalize_L2(embeddings)\n",
    "        self.vector_store.add(embeddings.astype('float32'))\n",
    "        \n",
    "        print(f\"‚úÖ Vector store created with {len(self.chunks)} documents\")\n",
    "    \n",
    "    def retrieve_relevant_chunks(self, query: str, k: int = 3) -> List[Tuple[str, float]]:\n",
    "        \"\"\"Retrieve most relevant chunks for the query\"\"\"\n",
    "        if not self.vector_store:\n",
    "            return []\n",
    "        \n",
    "        # Create query embedding\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        # Search for similar chunks\n",
    "        scores, indices = self.vector_store.search(query_embedding.astype('float32'), k)\n",
    "        \n",
    "        # Return chunks with scores\n",
    "        results = []\n",
    "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
    "            if idx < len(self.chunks):\n",
    "                results.append((self.chunks[idx], float(score)))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_answer(self, query: str, context_chunks: List[str]) -> str:\n",
    "        \"\"\"Generate answer using retrieved context\"\"\"\n",
    "        # Combine context\n",
    "        context = \"\\n\\n\".join(context_chunks[:3])  # Use top 3 chunks\n",
    "        \n",
    "        # Simple extractive approach - find most relevant sentences\n",
    "        sentences = context.split('.')\n",
    "        query_words = set(query.lower().split())\n",
    "        \n",
    "        # Score sentences based on query word overlap\n",
    "        scored_sentences = []\n",
    "        for sentence in sentences:\n",
    "            if len(sentence.strip()) > 20:  # Only consider substantial sentences\n",
    "                sentence_words = set(sentence.lower().split())\n",
    "                overlap = len(query_words.intersection(sentence_words))\n",
    "                if overlap > 0:\n",
    "                    scored_sentences.append((sentence.strip(), overlap))\n",
    "        \n",
    "        # Sort by score and take top sentences\n",
    "        scored_sentences.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        if scored_sentences:\n",
    "            # Combine top 2-3 most relevant sentences\n",
    "            answer_parts = [sent[0] for sent in scored_sentences[:3]]\n",
    "            answer = \". \".join(answer_parts)\n",
    "            if not answer.endswith('.'):\n",
    "                answer += \".\"\n",
    "            return answer\n",
    "        else:\n",
    "            # Fallback: return first part of context\n",
    "            return context[:500] + \"...\" if len(context) > 500 else context\n",
    "    \n",
    "    def answer_question(self, query: str) -> Dict[str, any]:\n",
    "        \"\"\"Complete QA pipeline\"\"\"\n",
    "        if not self.chunks:\n",
    "            return {\n",
    "                \"answer\": \"‚ùå No document loaded. Please upload a PDF first.\",\n",
    "                \"sources\": [],\n",
    "                \"confidence\": 0.0\n",
    "            }\n",
    "        \n",
    "        # Retrieve relevant chunks\n",
    "        relevant_chunks = self.retrieve_relevant_chunks(query, k=5)\n",
    "        \n",
    "        if not relevant_chunks:\n",
    "            return {\n",
    "                \"answer\": \"‚ùå No relevant information found in the document.\",\n",
    "                \"sources\": [],\n",
    "                \"confidence\": 0.0\n",
    "            }\n",
    "        \n",
    "        # Extract chunks and calculate average confidence\n",
    "        context_chunks = [chunk for chunk, _ in relevant_chunks]\n",
    "        avg_confidence = np.mean([score for _, score in relevant_chunks])\n",
    "        \n",
    "        # Generate answer\n",
    "        answer = self.generate_answer(query, context_chunks)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"sources\": context_chunks[:3],  # Top 3 sources\n",
    "            \"confidence\": float(avg_confidence)\n",
    "        }\n",
    "\n",
    "print(\"ü§ñ RAGSystem class created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Beautiful UI with Gradio\n",
    "\n",
    "Now let's create an attractive and user-friendly interface:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG system\n",
    "rag_system = RAGSystem()\n",
    "\n",
    "def upload_and_process_document(file):\n",
    "    \"\"\"Handle document upload and processing\"\"\"\n",
    "    if file is None:\n",
    "        return \"‚ùå Please upload a file (PDF, JSON, or TXT).\"\n",
    "    \n",
    "    # Check file extension\n",
    "    file_ext = os.path.splitext(file.name)[1].lower()\n",
    "    supported_formats = ['.pdf', '.json', '.txt']\n",
    "    \n",
    "    if file_ext not in supported_formats:\n",
    "        return f\"‚ùå Unsupported file format: {file_ext}. Please upload: {', '.join(supported_formats)}\"\n",
    "    \n",
    "    try:\n",
    "        # Process the uploaded file\n",
    "        rag_system.load_document(file.name)\n",
    "        return f\"‚úÖ {file_ext.upper()} document processed successfully! Ready to answer questions about: {os.path.basename(file.name)}\"\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error processing document: {str(e)}\"\n",
    "\n",
    "def answer_question(question, chat_history):\n",
    "    \"\"\"Handle question answering\"\"\"\n",
    "    if not question.strip():\n",
    "        return chat_history, \"\"\n",
    "    \n",
    "    # Get answer from RAG system\n",
    "    result = rag_system.answer_question(question)\n",
    "    \n",
    "    # Format response\n",
    "    answer = result[\"answer\"]\n",
    "    confidence = result[\"confidence\"]\n",
    "    \n",
    "    # Add confidence indicator\n",
    "    if confidence > 0.8:\n",
    "        confidence_emoji = \"üü¢\"\n",
    "    elif confidence > 0.6:\n",
    "        confidence_emoji = \"üü°\"\n",
    "    else:\n",
    "        confidence_emoji = \"üî¥\"\n",
    "    \n",
    "    formatted_answer = f\"{answer}\\n\\n{confidence_emoji} Confidence: {confidence:.2f}\"\n",
    "    \n",
    "    # Update chat history\n",
    "    chat_history.append([question, formatted_answer])\n",
    "    \n",
    "    return chat_history, \"\"\n",
    "\n",
    "def show_sources(question):\n",
    "    \"\"\"Show source chunks for transparency\"\"\"\n",
    "    if not question.strip():\n",
    "        return \"Please ask a question first.\"\n",
    "    \n",
    "    result = rag_system.answer_question(question)\n",
    "    sources = result[\"sources\"]\n",
    "    \n",
    "    if not sources:\n",
    "        return \"No sources found.\"\n",
    "    \n",
    "    formatted_sources = \"\\n\\n\" + \"=\"*50 + \"\\n\\n\".join(\n",
    "        [f\"üìÑ **Source {i+1}:**\\n{source[:300]}...\" for i, source in enumerate(sources)]\n",
    "    )\n",
    "    \n",
    "    return formatted_sources\n",
    "\n",
    "print(\"üéØ UI functions created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Launch the Application\n",
    "\n",
    "Let's create and launch our beautiful RAG chatbot interface:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CSS for beautiful styling\n",
    "custom_css = \"\"\"\n",
    ".gradio-container {\n",
    "    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "}\n",
    "\n",
    ".header {\n",
    "    text-align: center;\n",
    "    background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);\n",
    "    color: white;\n",
    "    padding: 20px;\n",
    "    border-radius: 10px;\n",
    "    margin-bottom: 20px;\n",
    "}\n",
    "\n",
    ".chat-container {\n",
    "    border: 2px solid #e1e5e9;\n",
    "    border-radius: 10px;\n",
    "    padding: 10px;\n",
    "}\n",
    "\n",
    ".upload-area {\n",
    "    border: 2px dashed #667eea;\n",
    "    border-radius: 10px;\n",
    "    padding: 20px;\n",
    "    text-align: center;\n",
    "    background-color: #f8f9fa;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Create the Gradio interface\n",
    "with gr.Blocks(css=custom_css, title=\"ü§ñ RAG Document QA Chatbot\") as demo:\n",
    "    \n",
    "    # Header\n",
    "    gr.HTML(\"\"\"\n",
    "    <div class=\"header\">\n",
    "        <h1>ü§ñ RAG-Based Document QA Chatbot</h1>\n",
    "        <p>Upload your documents (PDF, JSON, TXT) and ask questions! Powered by AI and semantic search.</p>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            gr.HTML(\"<h3>üìÑ Document Upload</h3>\")\n",
    "            \n",
    "            file_upload = gr.File(\n",
    "                label=\"Upload Document (PDF, JSON, TXT)\",\n",
    "                file_types=[\".pdf\", \".json\", \".txt\"],\n",
    "                elem_classes=[\"upload-area\"]\n",
    "            )\n",
    "            \n",
    "            upload_status = gr.Textbox(\n",
    "                label=\"üìä Status\",\n",
    "                interactive=False,\n",
    "                value=\"üîÑ Ready to upload document...\"\n",
    "            )\n",
    "            \n",
    "            process_btn = gr.Button(\n",
    "                \"üöÄ Process Document\",\n",
    "                variant=\"primary\",\n",
    "                size=\"lg\"\n",
    "            )\n",
    "            \n",
    "        with gr.Column(scale=2):\n",
    "            gr.HTML(\"<h3>üí¨ Chat with Your Document</h3>\")\n",
    "            \n",
    "            chatbot = gr.Chatbot(\n",
    "                label=\"ü§ñ AI Assistant\",\n",
    "                height=400,\n",
    "                elem_classes=[\"chat-container\"]\n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                question_input = gr.Textbox(\n",
    "                    label=\"üí≠ Ask a question\",\n",
    "                    placeholder=\"What would you like to know about your document?\",\n",
    "                    scale=4\n",
    "                )\n",
    "                ask_btn = gr.Button(\"üîç Ask\", variant=\"primary\", scale=1)\n",
    "            \n",
    "            with gr.Row():\n",
    "                clear_btn = gr.Button(\"üóëÔ∏è Clear Chat\", variant=\"secondary\")\n",
    "                sources_btn = gr.Button(\"üìö Show Sources\", variant=\"secondary\")\n",
    "    \n",
    "    # Sources section\n",
    "    with gr.Row():\n",
    "        sources_output = gr.Textbox(\n",
    "            label=\"üìö Source Documents\",\n",
    "            lines=8,\n",
    "            interactive=False,\n",
    "            visible=False\n",
    "        )\n",
    "    \n",
    "    # Event handlers\n",
    "    process_btn.click(\n",
    "        fn=upload_and_process_document,\n",
    "        inputs=[file_upload],\n",
    "        outputs=[upload_status]\n",
    "    )\n",
    "    \n",
    "    ask_btn.click(\n",
    "        fn=answer_question,\n",
    "        inputs=[question_input, chatbot],\n",
    "        outputs=[chatbot, question_input]\n",
    "    )\n",
    "    \n",
    "    question_input.submit(\n",
    "        fn=answer_question,\n",
    "        inputs=[question_input, chatbot],\n",
    "        outputs=[chatbot, question_input]\n",
    "    )\n",
    "    \n",
    "    clear_btn.click(\n",
    "        fn=lambda: ([], \"\"),\n",
    "        outputs=[chatbot, sources_output]\n",
    "    )\n",
    "    \n",
    "    def toggle_sources(question):\n",
    "        sources = show_sources(question)\n",
    "        return gr.update(value=sources, visible=True)\n",
    "    \n",
    "    sources_btn.click(\n",
    "        fn=toggle_sources,\n",
    "        inputs=[question_input],\n",
    "        outputs=[sources_output]\n",
    "    )\n",
    "    \n",
    "    # Footer\n",
    "    gr.HTML(\"\"\"\n",
    "    <div style=\"text-align: center; margin-top: 20px; color: #666;\">\n",
    "        <p>üîß Built with ‚ù§Ô∏è using Gradio, Transformers, and FAISS</p>\n",
    "        <p>üöÄ Supports PDF, JSON, TXT files ‚Ä¢ üß† AI-powered answers ‚Ä¢ üîç Semantic search</p>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "\n",
    "print(\"üé® Beautiful UI created!\")\n",
    "print(\"üöÄ Ready to launch the application!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the application\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Launching RAG Document QA Chatbot...\")\n",
    "    print(\"üì± The app will open in your browser automatically!\")\n",
    "    print(\"üîó You can also share the public link with others!\")\n",
    "    \n",
    "    demo.launch(\n",
    "        share=True,  # Create a public link\n",
    "        server_name=\"0.0.0.0\",  # Allow external connections\n",
    "        server_port=7860,  # Default Gradio port\n",
    "        show_error=True,  # Show errors in UI\n",
    "        debug=True  # Enable debug mode\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Alternative Free API Integration\n",
    "\n",
    "Here's how you can integrate with Google Gemini API (free tier) if you have an API key:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Google Gemini Integration\n",
    "# First install: !pip install google-generativeai\n",
    "\n",
    "\"\"\"\n",
    "import google.generativeai as genai\n",
    "\n",
    "class GeminiRAGSystem(RAGSystem):\n",
    "    def __init__(self, api_key: str):\n",
    "        super().__init__()\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel('gemini-pro')\n",
    "    \n",
    "    def generate_answer(self, query: str, context_chunks: List[str]) -> str:\n",
    "        context = \"\\n\\n\".join(context_chunks[:3])\n",
    "        \n",
    "        prompt = f\\\"\\\"\\\"Based on the following context from a document, please provide a comprehensive and accurate answer to the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Please provide a detailed answer based on the context provided. If the context doesn't contain enough information to answer the question, please state that clearly.\n",
    "\n",
    "Answer:\\\"\\\"\\\"\n",
    "        \n",
    "        try:\n",
    "            response = self.model.generate_content(prompt)\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            return f\"Error generating answer with Gemini: {str(e)}\"\n",
    "\n",
    "# To use Gemini, uncomment the following and add your API key:\n",
    "# GEMINI_API_KEY = \"your-gemini-api-key-here\"\n",
    "# rag_system = GeminiRAGSystem(GEMINI_API_KEY)\n",
    "\"\"\"\n",
    "\n",
    "print(\"üíé Gemini integration code ready (commented out)\")\n",
    "print(\"üîë Add your API key to use Gemini instead of the default model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Usage Instructions\n",
    "\n",
    "### How to Use the RAG Document QA Chatbot:\n",
    "\n",
    "1. **üìÑ Upload Document**: Click on the file upload area and select your PDF document\n",
    "2. **üöÄ Process Document**: Click the \"Process Document\" button to analyze and index your PDF\n",
    "3. **üí¨ Ask Questions**: Type your questions in the chat input and press Enter or click \"Ask\"\n",
    "4. **üìö View Sources**: Click \"Show Sources\" to see the document chunks used to generate answers\n",
    "5. **üóëÔ∏è Clear Chat**: Use the \"Clear Chat\" button to start a new conversation\n",
    "\n",
    "### Features:\n",
    "- ‚úÖ **Semantic Search**: Finds relevant information even if exact keywords don't match\n",
    "- ‚úÖ **Confidence Scoring**: Shows how confident the AI is in its answers\n",
    "- ‚úÖ **Source Transparency**: View the exact document sections used for answers\n",
    "- ‚úÖ **Beautiful UI**: Modern, responsive design with attractive styling\n",
    "- ‚úÖ **Free to Use**: No API keys required for basic functionality\n",
    "\n",
    "### Tips for Best Results:\n",
    "- üìù Ask specific, clear questions\n",
    "- üîç Try rephrasing questions if the answer isn't satisfactory\n",
    "- üìä Pay attention to confidence scores (üü¢ High, üü° Medium, üî¥ Low)\n",
    "- üìö Check sources to verify information\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Your RAG-based Document QA Chatbot is ready to use!**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
